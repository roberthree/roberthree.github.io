---
title: "Planning Complexity with Story Points"

subtitle: "Why Ideal Hours are an Anti-Pattern"
author: "Robert Three"
date: "01 December 2025"
categories: ["software development"]

format:
  html:
    code-fold: true
---

In software development, stakeholders inevitably demand forecasts for feature implementation, typically modeled via a set of distinct [(user) stories](https://en.wikipedia.org/wiki/User_story).
The perpetual question — _"When will it be done?"_ — dominates planning.
For the team, however, the relevant question is not _when_ but _can we answer "when" with statistically significant and repeatable accuracy?_

Most teams still estimate stories in ideal hours tied to individual developer capacity.
This approach is fundamentally flawed due to multiple reasons:

- _Skill variance_\
  The same story can take a senior developer two hours and a junior one ten. Hour-based estimates therefore depend on who does the work—information often unknown at planning time.
- _[Planning fallacy](https://en.wikipedia.org/wiki/Planning_fallacy)_\
  As detailed in [Daniel Kahneman’s Thinking, Fast and Slow (2011)](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow), humans display an optimism bias, consistently underestimating the time required for complex tasks by failing to account for context switching, interruptions, and unforeseen dependencies.
- _Non-linearity of effort ([Brooks's law](https://en.wikipedia.org/wiki/Brooks%27s_law))_\
  Time-based thinking assumes a linear relationship between person-hours and output.
  In reality, software development is a system of complex interdependencies where adding more hours (or people) does not linearly yield more features, often due to communication overhead and integration complexity.
- _Administrative overhead (time-logging)_\
  If time is the estimation metric, time-logging for every story is mandatory.
  This is not only an unnecessary administrative burden but also introduces a systematic source of uncertainty due to human error, logging mistakes, and the ethical dilemma of "billing" internal time.

To mitigate these systematic risks, **teams should decouple complexity from time**.
While time varies based on the worker and the environment, complexity remains an intrinsic property of the problem itself.

## Story points: Quantifying complexity

**Story points** are a relative unit of measure used to estimate the total effort required to satisfy a story's acceptance criteria. They **are not a proxy for hours**, but rather a scalar value derived from the inherent properties of the work:

- _(Technical) Difficulty:_ How hard is the problem?\
  E.g., A complex algorithm is harder than a simple [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) operation.
- _Volume / Scope:_ How much is there to do?\
  E.g., Changing a simple pattern in many files is low complexity but high volume.
- _Uncertainty (Risk):_ What do we not know?\
  E.g., Integrating with a legacy 3rd party API carries high risk compared to internal code.

To prevent false precision and mitigate cognitive biases, teams utilize non-linear scales, such as the [Fibonacci sequence](https://en.wikipedia.org/wiki/Fibonacci_sequence) (1, 2, 3, 5, 8, 13, ...) or [powers of two](https://en.wikipedia.org/wiki/Power_of_two) (1, 2, 4, 8, 16, 32, ...).
This non-linear scaling is rooted in the [Weber–Fechner law](https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law) of psychophysics, which states that “the minimum increase of stimulus which will produce a perceptible increase of sensation is proportional to the pre-existent stimulus”.
In estimation terms: We can easily distinguish the effort difference between a 1-point and a 2-point story.
However, distinguishing a 20 from a 21 is cognitively impossible.
The gaps in the mentioned sequences force decisive categorization, reducing analysis paralysis.
The team may even drastically reduce the available story points (e.g. (1, 3, 8)), to move the focus from story estimation to story refinement.

To ensure the complexity level is well-defined, the team must establish one or multiple reference stories, which are arbitrarily assigned 2 or 3 story points.
All future stories are compared against these references:
"Is this new story roughly the same size as the reference? Or is it twice as hard? Half as hard?".

## Velocity: Empirical Throughput Analysis

**Velocity** measures how much work a team actually completes in one iteration.
It is an empirical, historical metric — not a theoretical promise — and should be used to translate relative complexity (story points) into probabilistic feature forecasts.

While standard definitions of velocity just take the sum of completed story points, a more robust metric for forecasting is a capacity-adjusted velocity.
This approach accounts for the volatility of available working hours (vacations, holidays, on-call rotation) without reverting to hour-based estimation if it is kept coarse enough.
At the end of an iteration all story points of stories that met the acceptance criteria are summed up and are related to the team's capacity of this iteration:

$$\text{Velocity} = \frac{\text{Sum of story points of completed stories}}{\text{Team's coarse capacity}}$$

A capacity of 100% represents a full standard iteration.
However, it is important to keep the capacity factor coarse to avoid the early mentioned problems (skill variance or non-linearity of effort).
Attempting to calculate capacity down to the hour reintroduces the same errors time-based approaches suffer from.

After a few iterations the team is ready to make a reasonable forecast for the next iteration, assuming for a moment story point estimations have been correct.
The next iteration's possible story points is given by the the mean velocity of recent iterations ([moving average](https://en.wikipedia.org/wiki/Moving_average)) times the team's anticipated coarse capacity.

## Retrospective: Closing the Feedback Loop

⚠️ **Future planning is meaningless without a rigorous retrospective on estimation accuracy!**

After an iteration the team may realize that they accomplished less story points than planned.
There are three reasons why this may have happened:

1. _Unforeseen interruptions_\
  External events (incidents, urgent support, unplanned meetings) reduced available capacity mid-iteration.
  Record the interruption, treat the original story point estimates as still valid, and adjust the capacity used for future forecasts or ignore this iteration completely.
  If interruptions are recurring, add a predictable buffer to capacity rather than inflating story points.
2. _Incorrect story point estimates_\
  Work consistently takes more or less effort than similar reference stories.
  Such stories should be identified, analyzed and generic rules should be derived.
  For example, this could be due to a general underestimation of certain tasks, or a misinterpretation of the acceptance criteria.
  The derived rules should be applicable to many of other stories.
3. _Overestimated velocity_\
  Similar to (1.) this is automatically adapted in future iteration plannings.

## Velocity in the light of Goodhart's law

[Goodhart's law](https://en.wikipedia.org/wiki/Goodhart%27s_law) states:

> _Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes._ — Charles Goodhart (1975)

Or in other words:

> _When a measure becomes a target, it ceases to be a good measure._ — Strathern, Marilyn (1997).

Velocity is a calibration tool for planning, not a performance metric for remuneration.
If teams feel pressured to "increase" velocity, they will simply inflate story point values, rendering the metric useless for meaningful forecasts.

## When will it be done?

Recapitulation of previous sections:

- **Story points:** The team estimates the complexity of stories, not the duration.
- **Velocity:** The team evaluates accomplishable complexity over time via past performances.
- **Retrospective:** The team analyzes discrepancies between planned and accomplished complexity.

If the team has a high confidence in their recent velocity history and story estimation skills then it is now possible to come back to the initial question: "When will it be done?".
Or more precisely, when will a number of planned story points be done?

```{ojs}
//| echo: false

viewof planned_story_points = assignLabelWidth(
  Inputs.range(
    [0, 1000],
    {
      transform: Math.log,
      step: 1,
      value: 100,
      label: "Planned story points",
    }
  ),
)
```

Since we do not know the velocities of upcoming iterations, we have to provide a probability distribution based on the given knowledge.
While [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) from observed velocities is ideal, new teams often lack data.
In the case where data is scarce, another simple technique is the [three-point estimation](https://en.wikipedia.org/wiki/Three-point_estimation).
Since the uncertainty of the real distribution is high, it is reasonable to assume a simple [triangular distribution](https://en.wikipedia.org/wiki/Triangular_distribution).
The team must define three points for their future velocity:

```{ojs}
//| echo: false

function createVelocityInput() {
  let m = Inputs.range(domain, {step: 1, value: 20, label: "Most likely velocity"});
  let a = Inputs.range(domain, {step: 1, value: 0, label: "Worst-case velocity"});
  let b = Inputs.range(domain, {step: 1, value: 25, label: "Best-case velocity"});
  return {
    a: assignLabelWidth(a),
    m: assignLabelWidth(m),
    b: assignLabelWidth(b),
  }
}

viewof triangular = Inputs.form(createVelocityInput())

isVelocityInputValid = () => (triangular.a <= triangular.m) && (triangular.m <= triangular.b);

function ifValidVelocityInput(operation) {
  if (isVelocityInputValid()) {
    return operation()
  } else {
    return html`<div style="color:red; font-weight:bold;">
      ⚠️ Invalid velocity input (worst-case ≤ most-likely ≤ best-case)
    </div>`;
  }
}

ifValidVelocityInput(() => html``)
```

This results in a triangular distribution (see @fig-velocity-distribution in @sec-appendix) from which one can draw randomly velocity samples as depicted in @fig-velocity-sampling.
This approach is helpful if one wants to focus on a small number of recent velocities.
Another sampling technique is

```{ojs}
//| label: fig-velocity-sampling
//| fig-cap: "Random velocities of the team based on the given distribution"
//| echo: false

ifValidVelocityInput(() => pachinko(drawTriangular, domain))
```

Based on this sampling the team can simulate the number of iterations they need to accomplish all planned story points with a [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method):
New iterations are simulated with a randomly sampled velocity until all story points are accomplished.
Repeating this algorithm results in a distribution of possible number of iterations needed for the planned story points as depicted in @fig-forecast.

```{ojs}
//| label: fig-forecast
//| fig-cap: "Monte Carlo based forecast of number of iterations needed to accomplish planned story points"
//| echo: false

ifValidVelocityInput(
  () => {
    let domain = [0, 50];
    let data = Array.from(
      {length: num_simulations},
      () => ({needed_iterations: computeNeededIterations()}),
    );
    let thresholds = Array.from(
      {length: domain[1] - domain[0] + 1},
      (_, i) => domain[0] - 0.5 + i,
    );
    return Plot.plot({
      marks: [
        Plot.rectY(
          data,
          Plot.binX(
            {y: "count"},
            {x: "needed_iterations", thresholds},
          ),
        ),
        Plot.ruleY([0]),
      ],
      x: {domain, label: "Estimated number of needed iterations"},
      y: {label: "Amount"},
      height: 200,
      width,
    })
  }
)
```

Depending on the number of Monte Carlo simulation the resulting histogram in @fig-forecast may be less robust.

```{ojs}
//| echo: false

viewof num_simulations = assignLabelWidth(
  Inputs.range(
    [0, 100000],
    {
      transform: Math.log,
      step: 1,
      value: 10000,
      label: "Number of simulations",
    }
  )
)
```

## Further resources

- [Wikipedia: Cone of uncertainty](https://en.wikipedia.org/wiki/Cone_of_uncertainty)
- [Atlassian: Agile project management: Story points and estimation](https://www.atlassian.com/agile/project-management/estimation)

## Appendix {#sec-appendix}

```{ojs}
//| echo: false

domain = [0, 100]

function triangularPdf(x) {
  const [a, b, m] = [triangular.a, triangular.b, triangular.m]
  if (x < a || x > b) return 0;
  if (x <= m) return 2 * (x - a) / ((b - a) * (m - a));
  return 2 * (b - x) / ((b - a) * (b - m));
}

function plotPdf(pdf) {
  const xs = d3.range(domain[0], domain[1], (domain[1] - domain[0]) / 100)
  const data = xs.map(x => ({x, y: pdf(x)}))

  return Plot.plot({
    marks: [
      Plot.areaY(data, { x: "x", y: "y", opacity: 0.4 }),
      Plot.line(data, { x: "x", y: "y" })
    ],
    x: {domain, label: "Velocity (story points / iteration)"},
    y: {label: "Distribution Density"},
    height: 200,
    width,
  })
}

function drawTriangular() {
  const [a, b, m] = [triangular.a, triangular.b, triangular.m]

  let u = Math.random();

  // https://en.wikipedia.org/wiki/Triangular_distribution#Generating_random_variates
  if (u < (m - a) / (b - a)) {
    return a + Math.sqrt(u * (b - a) * (m - a));
  } else {
    return b - Math.sqrt((1 - u) * (b - a) * (b - m));
  }
}

function computeNeededIterations() {
  let remaining_story_points = planned_story_points
  let num_iterations = 0

  while (remaining_story_points > 0) {
    let velocity = drawTriangular()
    remaining_story_points -= velocity
    num_iterations += 1
  }

  return num_iterations
}

function assignLabelWidth(input) {
  return Object.assign(input, {style: '--label-width: 180px'});
}

import {pachinko} from "@d3/d3-random"
```

```{ojs}
//| label: fig-velocity-distribution
//| fig-cap: "Triangular distribution for the team's velocity"
//| echo: false

ifValidVelocityInput(() => plotPdf(triangularPdf))
```
